<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jierun CHEN (ÈôàÊç∑Ê∂¶)</title>
  
  <meta name="author" content="Jierun CHEN">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>

  <div style="padding:0;border:0;width:100%;position:absolute;top:0;left:0;">
    <a href="images/JierunCHEN.jpg" class="phone">
      <img src="images/JierunCHEN_phone.jpg" class="profile_img_top">
    </a>
  </div>
  
  <div style="padding:0;border:0;width:100%;position:relative;top:0;left:0;visibility: hidden;">
    <a href="images/JierunCHEN.jpg" class="phone">
      <img src="images/JierunCHEN_phone.jpg" class="profile_img_top">
    </a>
  </div>

  <table><tbody><tr style="padding:0px"><td style="padding:0px">
    <!-- ********************************* Profile ********************************* -->
    <table><tbody>
      <tr>
        <td class="profile_left desktop">
          <a href="images/JierunCHEN.jpg">
            <img src="images/JierunCHEN.jpg" class="profile_img_left">
          </a>
        </td>
        <td class="profile_right">
          <p style="text-align:center">
            <name>Jierun CHEN (ÈôàÊç∑Ê∂¶)</name>
          </p>
          <p>
            Hi! I am Jierun Chen, a final-year Ph.D. candidate in
            <a href="https://hkust.edu.hk/">HKUST</a>
            <a href="https://cse.hkust.edu.hk/">CSE Department</a>,
            advised by <a href="https://www.cse.ust.hk/~gchan/">Prof. Shueng-Han Gary Chan</a>. I interned at <a href="https://research.snap.com/">Snap Research</a> working on efficient text-to-image models, with 
		  <a href="https://scholar.google.com/citations?user=vDALiU4AAAAJ&hl=en">Jian Ren</a> and 
		  <a href="https://scholar.google.com/citations?user=bZdVsMkAAAAJ&hl=en">Anil Kag</a>. I also interned at
		  <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>,
	    working with <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en">Fangyun Wei</a> on multimodal large language models (MLLM).
            I received my B.Eng. in Electrical Engineering from Edison Experimental Class, 
            <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>.
          </p>
          <p>My research interests lie in foundation models, efficient deep learning, and their application to various computer vision tasks, with focuses on:
            <ul>
	      <li>Efficient AIGC, </li>
	      <li>Multimodal Large Language Models (MLLM), </li>
              <li>Mobile and Edge AI. </li>
            </ul>
          </p>
	  
<!-- 	  <p style="color:#FF0000";>
		I am currently active on the job market and seeking a position in the industry.
          </p> -->
          
          <p style="text-align:center">
            <a href="mailto:jierunchen@gmail.com">Email</a>: jierunchen@gmail.com &nbsp/&nbsp
            <a href="data/JierunCHEN_CV.pdf">CV</a> &nbsp/&nbsp
            <a href="https://scholar.google.com/citations?user=8rPHNOsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
            <a href="https://www.linkedin.com/in/jierunchen">LinkedIn</a> &nbsp/&nbsp
            <a href="https://github.com/JierunChen">Github</a>
          </p>
        </td>
      </tr> 

          
    <!-- ********************************* Publications ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
        </td>
      </tr>
    </tbody></table>
	  
    
    <table><tbody>
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <!-- <img src='images/snapgen_teaser.gif' class="pub_img"> -->
            <img src='images/snapgen_teaser.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training</papertitle>
          <br>
          <strong>Jierun Chen*</strong>, Dongting Hu*, Xijie Huang*, Huseyin Coskun, Arpit Sahni, Aarush Gupta, Anujraaj Goyal, Dishani Lahiri, Rajesh Singh, Yerlan Idelbayev, Junli Cao, Yanyu Li, Kwang-Ting Cheng, S.-H. Gary Chan, Mingming Gong, Sergey Tulyakov, Anil Kag, Yanwu Xu, Jian Ren
          </a>
          <br>
          <em>Preprint, 2024</em>
          <br>
          <a href="https://snap-research.github.io/snapgen/">project </a> /
          <a href="https://arxiv.org/pdf/2412.09619">pdf</a>
          <br>
          <p>We propose SnapGen, the first text-to-image model (379M) that can synthesize high-resolution images (1024x1024) on mobile devices in 1.4s, and achieve 0.66 on GenEval metric.</p>
        </td>
      </tr>	
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/ascan.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation</papertitle>
          <br>
          Anil Kag, Huseyin Coskun, <strong>Jierun Chen</strong>, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Jian Ren
          </a>
          <br>
          <em>NeurIPs, 2024</em>
          <br>
          <a href="https://snap-research.github.io/snap_image/">project </a> /
          <a href="https://arxiv.org/pdf/2411.04967">pdf</a>
          <br>
          <p>We introduce AsCAN, a hybrid neural network with asymmetric convolutional and transformer blocks, offering superior performance and efficiency across image recognition and generation tasks.</p>
        </td>
      </tr>	
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/Ref-L4.png' class="pub_img80">
          </div>
         </td>
        <td class="pub_right">
            <papertitle>Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models</papertitle>
          <br>
          <strong>Jierun Chen*</strong>, Fangyun Wei*, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S.-H. Gary Chan, Hongyang Zhang</a>
          <br>
          <em>Preprint, 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2406.16866">pdf</a> /
          <a href="https://openreview.net/forum?id=mfJifAlRMY#discussion">openreview</a> /
          <a href="https://github.com/JierunChen/Ref-L4">code</a> /
          <a href="https://huggingface.co/datasets/JierunChen/Ref-L4">dataset</a>  
          <br>
          <p>We clean the widely-adopted RefCOCO,+,g benchmarks and introduce Ref-L4, a New REC benchmark in the LMM Era.</p>
        </td>
      </tr>	
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/StableKD.png' class="pub_img80">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>StableKD: Breaking Inter-block Optimization Entanglement for Stable Knowledge Distillation</papertitle>
          <br>
          Shiu-hong Kao*, <strong>Jierun Chen*</strong>, S.-H. Gary Chan</a>
          <br>
          <em>Preprint, 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2312.13223">pdf</a>
          <br>
          <p>We propose StableKD, a simple and efficient Knowledge Distillation framework that attains higher accuracy using fewer training epochs and less data.</p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/fasternet.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks</papertitle>
          <br>
          <strong>Jierun Chen</strong>, Shiu-hong Kao, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee, S.-H. Gary Chan  
          <br>
          <em>CVPR, 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2303.03667">pdf</a> /
          <a href="https://github.com/JierunChen/FasterNet">code</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JierunChen&repo=FasterNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>	
          <br>
          <p>We propose a simple yet fast and effective partial convolution (PConv), 
            as well as a latency-efficient family of architectures called FasterNet.</p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr onmouseout="tvconv_stop()" onmouseover="tvconv_start()">
        <td class="pub_left">
          <div class="one">
            <img src='images/TVConv.gif' class="pub_img70">
          </div>
        </td>
        <td class="pub_right">
          
            <papertitle>TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing</papertitle>
         
          <br>
          <strong>Jierun Chen</strong>, Tianlang He, Weipeng Zhuo, Li Ma, Sangtae Ha, S.-H. Gary Chan
          <br>
          <em>CVPR, 2022</em>
          <br>
          <a href="https://arxiv.org/pdf/2203.10489">pdf</a> / 
          <a href="https://www.youtube.com/watch?v=-kh2E21suKE">video</a> /
          <a href="https://github.com/JierunChen/TVConv">code</a>							
          <br>
          <p>TVConv works more computation-efficient than regular convolution when dealing with layout-specific 
            tasks, e.g., face recognition.</p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/Wild-JDD_cover.jpg' class="pub_img70">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty</papertitle>
          <br>
          <strong>Jierun Chen</strong>, Song Wen, S.-H. Gary Chan</a>
          <br>
          <em>AAAI, 2021</em>
          <br>
          <a href="https://arxiv.org/pdf/2101.04442">pdf</a> /
          <a href="https://slideslive.com/38948037/joint-demosaicking-and-denoising-in-the-wild-the-case-of-training-under-ground-truth-uncertainty">video</a>
          <br>
          <p>We consider the ground truth uncertainty for joint demosaicking and denoising in the wild, which provides better restoration result and interpretability.</p>
        </td>
      </tr>	
      <!-- ********************************* -->
      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/TASFAR.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>Target-agnostic Source-free Domain Adaptation for Regression Tasks</papertitle>
          <br>
	        Tianlang He, Zhiqiu Xia, <strong>Jierun Chen</strong>, Haoliang Li, S.-H. Gary Chan</a>
          <br>
          <em>ICDE, 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2312.00540">pdf</a>
          <br>
          <p>We propose TASFAR, a novel target-agnostic source-free domain adaptation method for regression tasks.</p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr onmouseout="CPNeRF_stop()" onmouseover="CPNeRF_start()">
        <td class="pub_left">
          <div class="one">
            <video  width=100% height=100% muted autoplay loop>
              <source src="images/CP-NeRF.mp4" type="video/mp4">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>CP-NeRF: Conditionally Parameterized Neural Radiance Fields for Cross-scene Novel View Synthesis</papertitle>
	        <br>
          Hao He, Yixun Liang, Shishi Xiao, <strong>Jierun Chen</strong>, Yingcong Chen</a>
          <br>
          <em>Pacific Graphics, 2023</em>
          <br>
          <a href="https://diglib.eg.org/server/api/core/bitstreams/f8249253-46eb-4f4c-86a1-1fcf3ee8f5fb/content">pdf</a>
          <p>
          We propose CP-NeRF to enable training a one-for-all NeRF across diverse scenes.
          </p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr onmouseout="fis_start()" onmouseover="fis_stop()">
        <!-- <td class="pub_left" >
          <div class="one">
            <div class="two" id='fis'>
              <img src='images/fis_0.png' class="pub_img60">
            </div>
            <img src='images/fis_1.png' class="pub_img60">
          </div>
          <script type="text/javascript">
            function fis_start() {
              document.getElementById('fis').style.opacity = "1";
            }

            function fis_stop() {
              document.getElementById('fis').style.opacity = "0";
            }
            fis_start()
          </script>
        </td> -->
        <td class="pub_left">
          <div class="one">
            <img src='images/fis_0.png' class="pub_img70">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals</papertitle>
          <br>
          Weipeng Zhuo, Ka Ho Chiu, <strong>Jierun Chen</strong>, Ziqi Zhao, S.-H. Gary Chan, Sangtae Ha, Chul-Ho Lee
          <br>
          <em>ICDCS, 2023</em>
          <br>          
          <a href="https://arxiv.org/pdf/2307.05914">pdf</a> /
          <a href="https://github.com/SteveZhuo/FIS-ONE">code</a>
          <br>        
          <p>We design a floor identification system for crowdsourced RF signals in a building using only one labeled data sample 
          from the bottom floor.</p>
        </td>
      </tr>
      <!-- ********************************* -->
      <tr onmouseout="gem_start()" onmouseover="gem_stop()">
        <!-- <td class="pub_left">
          <div class="one">
            <div class="two" id='gem'>
              <img src='images/GEM_0.png' class="pub_img60">
            </div>
            <img src='images/GEM_1.png' class="pub_img60">
          </div>
          <script type="text/javascript">
            function gem_start() {
              document.getElementById('gem').style.opacity = "1";
            }

            function gem_stop() {
              document.getElementById('gem').style.opacity = "0";
            }
            gem_start()
          </script>
        </td> -->
        <td class="pub_left">
          <div class="one">
            <img src='images/GEM_0.png' class="pub_img80">
          </div>
        </td>
        <td class="pub_right">
            <papertitle>Semi-supervised Learning with Network Embedding on Ambient RF Signals for Geofencing Services</papertitle>
          <br>
          Weipeng Zhuo, Ka Ho Chiu, <strong>Jierun Chen</strong>, Jiajie Tan, Edmund Sumpena, Sangtae Ha, S.-H. Gary Chan, Chul-Ho Lee
          <br>
          <em>ICDE, 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2210.07889">pdf</a> /
          <a href="https://github.com/SteveZhuo/bisage_geofencing">code</a>	
          <br>
          <p>We develop a practical geofencing system, solely based on ambient radio frequency (RF) signals,
              to enable applications like elderly care, dementia antiwandering, pandemic control, etc.</p>
        </td>
      </tr>

    </tbody></table>
	

    
    
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Misc</heading>
          <!-- ********************************* Selected Awards ********************************* -->
          <p>
            <strong>Selected Awards</strong> 
            <ul>
              <li>RedBird Academic Excellence Award, HKUST, 2022</li>
              <li>Postgraduate Scholarship, HKUST, 2019-2023</li>
              <li>Outstanding Graduate, ZJU, 2018</li>
              <li>Scholarship for Excellence in Research and Innovation, ZJU, 2017</li>
              <li>1st prize in Nanjiang Lebo Cup Provincial Robot Competition (ranked 1/62), 2017</li>
              <li>1st prize in National Undergraduate Electronic Design Contest (ranked 3/109 in Zhejiang Division), 2017</li>
              <li>Scholarship for Outstanding Merits, ZJU, 2015, 2017</li>
              <li>Excellent Student Awards, ZJU, 2015, 2017</li>
            </ul>
          </p>
          <!-- ********************************* Teaching ********************************* -->
          <p>
            <strong>Teaching Assistant</strong>
            <ul>
              <li>COMP 2012H Honors Object-Oriented Programming and Data Structures, Fall 2022</li>
              <li>COMP 4911/6613D/ENTR4911 IT Entrepreneurship, Fall 2021</li>
              <li>COMP 4021 Internet Computing, Fall 2020</li>
              <li>COMP 4611 Design and Analysis of Computer Architectures, Spring 2020</li>
            </ul>            
          </p>
          <!-- ********************************* Academic Services ********************************* -->
          <p>
            <strong>Reviewer</strong>
            <ul>
              <li>Conference: ICLR, CVPR, ECCV, ICCV, UbiComp, INFOCOM, BMVC, ACCV</li>
              <li>Journal: IJCV, TIM</li>
            </ul>
          </p>
         
          <!-- ********************************* Mentoring ********************************* -->
          <!-- <p>
            Mentoring:
            <ul>
              <li>UROP 1000/1100/2100/3100, Spring 2023/Fall 2022/Spring 2022/Fall 2021/Fall 2020/Summer 2020/Spring 2020</li>
              <li>MSBD 5014/CSIT 6910 Independent Project, Spring 2022/Fall 2021</li>
              <li>COMP 4981/4981H Final Year Projects/Final Year Theses, Spring 2023/Fall 2022/Fall 2021</li>
            </ul>
          </p>
           -->
        </td>
      </tr>
    </tbody></table>


    <div id="footer">        
      <p>
        <center>
          <div id="clustrmaps-widget" style="width:10%">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=n4oCPWnTsKMURBryd2FpxnLGLrc2pOsn8o1pbaYqI0E"></script>
          </div>
          &copy; Jierun Chen | Last updated: Dec 2024.              
        </center>
        <center>
          <a href="https://github.com/jonbarron/jonbarron_website">Website template.</a>
        </center>
      </p>      
    </div>
  </td></tr></tbody></table>
</body>

</html>
