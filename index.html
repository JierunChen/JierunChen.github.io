<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jierun CHEN (ÈôàÊç∑Ê∂¶)</title>
  
  <meta name="author" content="Jierun CHEN">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>

  <div style="padding:0;border:0;width:100%;position:absolute;top:0;left:0;">
    <a href="images/JierunCHEN.jpg" class="phone">
      <img src="images/JierunCHEN_phone.jpg" class="profile_img_top">
    </a>
  </div>
  
  <div style="padding:0;border:0;width:100%;position:relative;top:0;left:0;visibility: hidden;">
    <a href="images/JierunCHEN.jpg" class="phone">
      <img src="images/JierunCHEN_phone.jpg" class="profile_img_top">
    </a>
  </div>

  <table><tbody><tr style="padding:0px"><td style="padding:0px">
    <!-- ********************************* Profile ********************************* -->
    <table><tbody>
      <tr>
        <td class="profile_left desktop">
          <a href="images/JierunCHEN.jpg">
            <img src="images/JierunCHEN.jpg" class="profile_img_left">
          </a>
        </td>
        <td class="profile_right">
          <p style="text-align:center">
            <name>Jierun CHEN (ÈôàÊç∑Ê∂¶)</name>
          </p>
          <p>
            Hi! I am Jierun Chen, a final-year Ph.D. candidate in
            <a href="https://hkust.edu.hk/">HKUST</a>
            <a href="https://cse.hkust.edu.hk/">CSE Department</a>,
            advised by <a href="https://www.cse.ust.hk/~gchan/">Prof. Shueng-Han Gary Chan</a>. 
	    I am currently a research intern at <a href="https://research.snap.com/">Snap Research</a> working on efficient AIGC models, with 
		  <a href="https://research.snap.com/team/team-member.html#jian-ren">Jian Ren</a> and 
		  <a href="https://research.snap.com/team/team-member.html#anil-kag">Anil Kag</a>. Before that, I interned at
		  <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>,
	    working with <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en">Fangyun Wei</a> on multimodal large language models (MLLM).
            I received my B.Eng. in Electrical Engineering from Edison Experimental Class, 
            <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>.
          </p>
          <p>My research interests lie in foundation models, efficient deep learning, and their application to various computer vision tasks, with focuses on:
            <ul>
	      <li>Efficient AIGC, </li>
	      <li>Multimodal Large Language Models (MLLM), </li>
              <li>Mobile and Edge AI. </li>
            </ul>
          </p>
	  
<!-- 	  <p style="color:#FF0000";>
		I am currently active on the job market and seeking a position in the industry.
          </p> -->
          
          <p style="text-align:center">
            <a href="mailto:jierunchen@gmail.com">Email: jierunchen@gmail.com</a> &nbsp/&nbsp
            <a href="data/JierunCHEN-CV.pdf">CV</a> &nbsp/&nbsp
            <a href="https://scholar.google.com/citations?user=8rPHNOsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
            <a href="https://www.linkedin.com/in/jierunchen">LinkedIn</a> &nbsp/&nbsp
            <a href="https://github.com/JierunChen">Github</a>
          </p>
        </td>
      </tr> 

	<!-- ********************************* Preprints ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Preprints</heading>
        </td>
      </tr>
    </tbody></table>
      
    <table><tbody>
      <tr>
	<td class="pub_left">
	  <div class="one">
	    <img src='images/Ref-L4.png' class="pub_img">
	  </div>
	</td>
	<td class="pub_right">
	  <a href="data/Ref-L4.pdf">
	    <papertitle>Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models</papertitle>
	  </a>
	  <br>
	  <strong>Jierun Chen*</strong>, Fangyun Wei*, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S.-H. Gary Chan, Hongyang Zhang</a>
	  <br>
	  <em>Under review, 2024</em>
	  <br>
	  <a href="data/Ref-L4.pdf">paper</a> /
	  <a href="https://github.com/JierunChen/Ref-L4">code</a> /
	  <a href="https://huggingface.co/datasets/JierunChen/Ref-L4">dataset</a> /  
	  <br>
	  <p>We clean the widely-adopted RefCOCO,+,g benchmarks and introduce Ref-L4, a New REC benchmark in the LMM Era.</p>
	</td>
       </tr>	    
    </tbody></table>
	
    <table><tbody>
      <tr>
	<td class="pub_left">
	  <div class="one">
	    <img src='images/StableKD.png' class="pub_img">
	  </div>
	</td>
	<td class="pub_right">
	  <a href="https://arxiv.org/abs/2312.13223">
	    <papertitle>StableKD: Breaking Inter-block Optimization Entanglement for Stable Knowledge Distillation</papertitle>
	  </a>
	  <br>
	  Shiu-hong Kao*, <strong>Jierun Chen*</strong>, S.-H. Gary Chan</a>
	  <br>
	  <em>arxiv:2312.13223, 2023</em>
	  <br>
	  <a href="https://arxiv.org/abs/2312.13223">paper</a>
	  <br>
	  <p>We propose StableKD, a simple and efficient Knowledge Distillation framework that attains higher accuracy using fewer training epochs and less data.</p>
	</td>
       </tr>	    
    </tbody></table>
    
    <!-- ********************************* Publications ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
        </td>
      </tr>
    </tbody></table>
    
    <table><tbody>

      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/PConv.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
          <a href="https://arxiv.org/abs/2303.03667">
            <papertitle>Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks</papertitle>
          </a>
          <br>
          <strong>Jierun Chen</strong>, Shiu-hong Kao, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee, S.-H. Gary Chan  
          <br>
          <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2303.03667">paper</a> /
          <a href="https://github.com/JierunChen/FasterNet">code</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JierunChen&repo=FasterNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>	
          <br>
          <p>We propose a simple yet fast and effective partial convolution (PConv), 
            as well as a latency-efficient family of architectures called FasterNet.</p>
        </td>
      </tr>

      <tr onmouseout="tvconv_stop()" onmouseover="tvconv_start()">
        <td class="pub_left">
          <div class="one">
            <div class="two" id='tvconv_image'><img src='images/TVConv.gif' class="pub_img"></div>
            <img src='images/TVConv.jpg' class="pub_img">
            </div>
          </div>
          <script type="text/javascript">
            function tvconv_start() {
              document.getElementById('tvconv_image').style.opacity = "1";
            }

            function tvconv_stop() {
              document.getElementById('tvconv_image').style.opacity = "0";
            }
            tvconv_stop()
          </script>
        </td>
        <td class="pub_right">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_TVConv_Efficient_Translation_Variant_Convolution_for_Layout-Aware_Visual_Processing_CVPR_2022_paper.html">
            <papertitle>TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing</papertitle>
          </a>
          <br>
          <strong>Jierun Chen</strong>, Tianlang He, Weipeng Zhuo, Li Ma, Sangtae Ha, S.-H. Gary Chan
          <br>
          <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</em>
          <br>
          <a href="https://arxiv.org/abs/2203.10489">paper</a> / 
          <a href="https://www.youtube.com/watch?v=-kh2E21suKE">video</a> /
          <a href="https://github.com/JierunChen/TVConv">code</a>							
          <br>
          <p>TVConv works more computation-efficient than regular convolution when dealing with layout-specific 
            tasks, e.g., face recognition.</p>
        </td>
      </tr>

      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/Wild-JDD_cover.jpg' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16186">
            <papertitle>Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty</papertitle>
          </a>
          <br>
          <strong>Jierun Chen</strong>, Song Wen, S.-H. Gary Chan</a>
          <br>
          <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021</em>
          <br>
          <a href="https://arxiv.org/abs/2101.04442">paper</a> /
          <a href="https://slideslive.com/38948037/joint-demosaicking-and-denoising-in-the-wild-the-case-of-training-under-ground-truth-uncertainty">video</a>
          <br>
          <p>We consider the ground truth uncertainty for joint demosaicking and denoising in the wild, which provides better restoration result and interpretability.</p>
        </td>
      </tr>	

      <tr>
        <td class="pub_left">
          <div class="one">
            <img src='images/TASFAR.png' class="pub_img">
          </div>
        </td>
        <td class="pub_right">
          <a href="https://arxiv.org/abs/2312.00540">
            <papertitle>Target-agnostic Source-free Domain Adaptation for Regression Tasks</papertitle>
          </a>
          <br>
	  Tianlang He, Zhiqiu Xia, <strong>Jierun Chen</strong>, Haoliang Li, S.-H. Gary Chan</a>
          <br>
          <em>IEEE International Conference on Data Engineering (ICDE), 2024</em>
          <br>
          <a href="https://arxiv.org/abs/2312.00540">paper</a>
          <br>
          <p>We propose TASFAR, a novel target-agnostic source-free domain adaptation method for regression tasks.</p>
        </td>
      </tr>
    

      <tr onmouseout="CPNeRF_stop()" onmouseover="CPNeRF_start()">
        <td class="pub_left">
          <div class="one">
            <div class="two" id='CPNeRF_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/CP-NeRF.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/CP-NeRF.jpg'  class="pub_img">
          </div>
          <script type="text/javascript">
            function CPNeRF_start() {
              document.getElementById('CPNeRF_image').style.opacity = "1";
            }

            function CPNeRF_stop() {
              document.getElementById('CPNeRF_image').style.opacity = "0";
            }
            CPNeRF_stop()
          </script>
        </td>
        <td class="pub_right">
          <a href="">
            <papertitle>CP-NeRF: Conditionally Parameterized Neural Radiance Fields for Cross-scene Novel View Synthesis</papertitle>
          </a>
	  <br>
          Hao He, Yixun Liang, Shishi Xiao, <strong>Jierun Chen</strong>, Yingcong Chen</a>
          <br>
          <em>The 31th Pacific Conference on Computer Graphics and Applications (Pacific Graphics), 2023</em>
          <br>
          <a>paper and code coming soon</a>
          <p>
          We propose CP-NeRF to enable training a one-for-all NeRF across diverse scenes.
          </p>
        </td>
      </tr>

	    
      <tr>
        <td class="pub_left" onmouseout="fis_start()" onmouseover="fis_stop()">
          <div class="one">
            <div class="two" id='fis'>
              <img src='images/fis_0.png' class="pub_img">
            </div>
            <img src='images/fis_1.png' class="pub_img">
          </div>
          <script type="text/javascript">
            function fis_start() {
              document.getElementById('fis').style.opacity = "1";
            }

            function fis_stop() {
              document.getElementById('fis').style.opacity = "0";
            }
            fis_start()
          </script>
        </td>
        <td class="pub_right">
          <a href="https://arxiv.org/abs/2307.05914">
            <papertitle>FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals</papertitle>
          </a>
          <br>
          Weipeng Zhuo, Ka Ho Chiu, <strong>Jierun Chen</strong>, Ziqi Zhao, S.-H. Gary Chan, Sangtae Ha, Chul-Ho Lee
          <br>
          <em>IEEE International Conference on Distributed Computing Systems (ICDCS), 2023</em>
          <br>          
          <a href="https://arxiv.org/abs/2307.05914">paper</a> /
          <a href="https://github.com/SteveZhuo/FIS-ONE">code</a>
          <br>        
          <p>We design a floor identification system for crowdsourced RF signals in a building using only one labeled data sample 
          from the bottom floor.</p>
        </td>
      </tr>


      <tr onmouseout="gem_start()" onmouseover="gem_stop()">
        <td class="pub_left">
          <div class="one">
            <div class="two" id='gem'>
              <img src='images/GEM_0.png' class="pub_img">
            </div>
            <img src='images/GEM_1.png' class="pub_img">
          </div>
          <script type="text/javascript">
            function gem_start() {
              document.getElementById('gem').style.opacity = "1";
            }

            function gem_stop() {
              document.getElementById('gem').style.opacity = "0";
            }
            gem_start()
          </script>
        </td>
        <td class="pub_right">
          <a href="https://arxiv.org/abs/2210.07889">
            <papertitle>Semi-supervised Learning with Network Embedding on Ambient RF Signals for Geofencing Services</papertitle>
          </a>
          <br>
          Weipeng Zhuo, Ka Ho Chiu, <strong>Jierun Chen</strong>, Jiajie Tan, Edmund Sumpena, Sangtae Ha, S.-H. Gary Chan, Chul-Ho Lee
          <br>
          <em>IEEE International Conference on Data Engineering (ICDE), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2210.07889">paper</a> /
          <a href="https://github.com/SteveZhuo/bisage_geofencing">code</a>	
          <br>
          <p>We develop a practical geofencing system, solely based on ambient radio frequency (RF) signals,
              to enable applications like elderly care, dementia antiwandering, pandemic control, etc.</p>
        </td>
      </tr>


      
  
    </tbody></table>
	
    <!-- ********************************* Selected Awards ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Selected Awards</heading>
          <p>
            <ul>
              <li>RedBird Academic Excellence Award, HKUST, 2022</li>
              <li>Postgraduate Scholarship, HKUST, 2019-2023</li>
              <li>Outstanding Graduate, ZJU, 2018</li>
              <li>Scholarship for Excellence in Research and Innovation, ZJU, 2017</li>
              <li>1st prize in Nanjiang Lebo Cup Provincial Robot Competition (ranked 1/62), 2017</li>
              <li>1st prize in National Undergraduate Electronic Design Contest (ranked 3/109 in Zhejiang Division), 2017</li>
              <li>Scholarship for Outstanding Merits, ZJU, 2015, 2017</li>
              <li>Excellent Student Awards, ZJU, 2015, 2017</li>
            </ul>
          </p>
        </td>
      </tr>
    </tbody></table>
    
    <!-- ********************************* Teaching ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Teaching</heading>
          <p>
            Teaching Assistant:
            <ul>
              <li>COMP 2012H Honors Object-Oriented Programming and Data Structures, Fall 2022</li>
              <li>COMP 4911/6613D/ENTR4911 IT Entrepreneurship, Fall 2021</li>
              <li>COMP 4021 Internet Computing, Fall 2020</li>
              <li>COMP 4611 Design and Analysis of Computer Architectures, Spring 2020</li>
            </ul>            
          </p>

          <p>
            Mentoring:
            <ul>
              <li>UROP 1000/1100/2100/3100, Spring 2023/Fall 2022/Spring 2022/Fall 2021/Fall 2020/Summer 2020/Spring 2020</li>
              <li>MSBD 5014/CSIT 6910 Independent Project, Spring 2022/Fall 2021</li>
              <li>COMP 4981/4981H Final Year Projects/Final Year Theses, Spring 2023/Fall 2022/Fall 2021</li>
            </ul>
          </p>
          
        </td>
      </tr>
    </tbody></table>

    <!-- ********************************* Academic Services ********************************* -->
    <table><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Academic Services</heading>
          <p>
            <strong>Reviewer:</strong> CVPR 2023 - 2024, ECCV 2024, ICCV 2023, UbiComp2023, BMVC 2023, INFOCOM 2020 - 2023, BMVC 2020 - 2021
          </p>
        </td>
      </tr>
    </tbody></table>       

    <div id="footer">        
      <p>
        <center>
          <div id="clustrmaps-widget" style="width:10%">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=n4oCPWnTsKMURBryd2FpxnLGLrc2pOsn8o1pbaYqI0E"></script>
          </div>
          &copy; Jierun Chen | Last updated: Dec 2023.              
        </center>
        <center>
          <a href="https://github.com/jonbarron/jonbarron_website">Website template.</a>
        </center>
      </p>      
    </div>
  </td></tr></tbody></table>
</body>

</html>
